{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL Interface\n",
    "\n",
    "- Author: [JeongGi Park](https://github.com/jeongkpa)\n",
    "- Peer Review: [YooKyung Jeon](https://github.com/sirena1), [Wooseok Jeong](https://github.com/jeong-wooseok)\n",
    "- Proofread : [Q0211](https://github.com/Q0211)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/01-Basic/07-LCEL-Interface.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/01-Basic/07-LCEL-Interface.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "The LangChain Expression Language (LCEL) is a powerful interface designed to simplify the creation and management of custom chains in LangChain. \n",
    "It implements the Runnable protocol, providing a standardized way to build and execute language model chains.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [LCEL Runnable Protocol](#lcel-runnable-protocol)\n",
    "- [stream: real-time output](#stream-real-time-output)\n",
    "- [Invoke](#invoke)\n",
    "- [batch: unit execution](#batch-unit-execution)\n",
    "- [async stream](#async-stream)\n",
    "- [async invoke](#async-invoke)\n",
    "- [async batch](#async-batch)\n",
    "- [Parallel](#parallel)\n",
    "- [Parallelism in batches](#parallelism-in-batches)\n",
    "\n",
    "### References\n",
    "\n",
    "- [Langsmith DOC](https://docs.smith.langchain.com/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set environment variables is in .env.\n",
    "\n",
    "Copy the contents of .env_sample and load it into your .env with the key you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "!pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain-openai\",\n",
    "        \"langchain\",\n",
    "        \"python-dotenv\",\n",
    "        \"langchain-core\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        # \"OPENAI_API_KEY\": \"<Your OpenAI API KEY>\",\n",
    "        # \"LANGCHAIN_API_KEY\": \"<Your LangChain API KEY>\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"LangSmith-Tracking-Setup\",  # title 과 동일하게 설정해 주세요\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL Runnable Protocol\n",
    "\n",
    "---\n",
    "\n",
    "To make it as easy as possible to create custom chains, we've implemented the ```Runnable``` protocol.\n",
    "\n",
    "The ```Runnable``` protocol is implemented in most components.\n",
    "\n",
    "It is a standard interface that makes it easy to define custom chains and call them in a standard way. The standard interface includes\n",
    "\n",
    "- ```stream```: Streams a chunk of the response.\n",
    "- ```invoke```: Invoke a chain on an input.\n",
    "- ```batch```: Invoke a chain against a list of inputs.\n",
    "\n",
    "There are also asynchronous methods\n",
    "\n",
    "- ```astream```: Stream chunks of the response asynchronously.\n",
    "- ```ainvoke```: Invoke a chain asynchronously on an input.\n",
    "- ```abatch```: Asynchronously invoke a chain against a list of inputs.\n",
    "- ```astream_log```: Streams the final response as well as intermediate steps as they occur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log your trace\n",
    "\n",
    "We provide multiple ways to log traces to LangSmith. Below, we'll highlight how to use traceable().\n",
    "\n",
    "Use the code below to record a trace in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langsmith import wrappers, traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "client = wrappers.wrap_openai(openai.Client())\n",
    "\n",
    "@traceable # Auto-trace this function\n",
    "def pipeline(user_input: str):\n",
    "    result = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    return result.choices[0].message.content\n",
    "\n",
    "pipeline(\"Hello, world!\")\n",
    "# Out:  Hello there! How can I assist you today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a chain using LCEL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Instantiate the ChatOpenAI model.\n",
    "model = ChatOpenAI()\n",
    "# Create a prompt template that asks for jokes on a given topic.\n",
    "prompt = PromptTemplate.from_template(\"Describe the {topic} in 3 sentences.\")\n",
    "# Connect the prompt and model to create a conversation chain.\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stream: real-time output\n",
    "\n",
    "This function uses the ```chain.stream``` method to create a stream of data for a given topic, iterating over it and immediately outputting the ```content``` of each piece of data. \n",
    "The ```end=\"\"``` argument disables newlines after output, and the ```flush=True``` argument causes the output buffer to be emptied immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal refers to the use of multiple modes of communication, such as visual, auditory, and kinesthetic, to convey information. It emphasizes the importance of engaging multiple senses to enhance understanding and retention of information. By incorporating various modes of communication, multimodal approaches can cater to diverse learning styles and preferences."
     ]
    }
   ],
   "source": [
    "# Use the chain.stream method to create a stream of data for a given topic, iterating over it and immediately outputting the content of each piece of data. \n",
    "for token in chain.stream({\"topic\": \"multimodal\"}):\n",
    "    # Output the content of each piece of data without newlines.\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "# example output \n",
    "# The multimodal approach involves using multiple modes of communication, such as visual, auditory, and kinesthetic, to enhance learning and understanding. By incorporating different sensory inputs, learners are able to engage with material in a more holistic and immersive way. This approach is especially effective in catering to diverse learning styles and preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke\n",
    "\n",
    "The ```invoke``` method of a ```chain``` object takes a topic as an argument and performs processing on that topic.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ChatGPT is an AI-powered chatbot that uses natural language processing to engage in conversations with users. It can provide information, answer questions, and generate responses based on the input it receives. ChatGPT is constantly learning and improving its capabilities through machine learning algorithms.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the invoke method of the chain object, passing a dictionary with the topic 'ChatGPT'.\n",
    "chain.invoke({\"topic\": \"ChatGPT\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch: unit execution\n",
    "\n",
    "The function ```chain.batch``` takes a list containing multiple dictionaries as arguments and performs batch processing using the values of the ```topic``` key in each dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatGPT is an AI-powered chatbot that can engage in natural, fluid conversations with users. It uses advanced natural language processing algorithms to understand and respond to a wide range of topics. ChatGPT is designed to provide personalized and helpful responses to user inquiries, creating a seamless and interactive chat experience.',\n",
       " 'Instagram is a popular social media platform where users can share photos and videos with their followers. The app also offers features like filters, stories, and direct messaging to enhance the user experience. Many influencers, celebrities, and brands use Instagram to connect with their audience and showcase their content.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call a function to batch process a given list of topics\n",
    "chain.batch([{\"topic\": \"ChatGPT\"}, {\"topic\": \"Instagram\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the ```max_concurrency``` parameter to set the number of concurrent requests.\n",
    "\n",
    "The ```config``` dictionary uses the ```max_concurrency``` key to set the maximum number of operations that can be processed concurrently. Here, it is set to process up to three jobs concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatGPT is an AI-powered chatbot that uses natural language processing to engage in conversations with users. It is trained on a diverse range of topics and can provide helpful and informative responses. ChatGPT is constantly learning and improving its ability to understand and generate human-like conversations.',\n",
       " 'Instagram is a popular social media platform where users can share photos and videos with their followers. It allows users to apply filters and edit their posts before sharing them. Users can also engage with others by liking, commenting, and sharing posts.',\n",
       " 'The multimodal approach involves using multiple modes of communication, such as visual, auditory, and kinesthetic, to convey information. This approach recognizes that individuals have different learning styles and preferences, and aims to cater to a variety of sensory inputs. By incorporating multiple modes of communication, the multimodal approach can enhance understanding, engagement, and retention of information for learners.',\n",
       " 'Programming is the process of creating a set of instructions that tell a computer how to perform a specific task. It involves writing code using a programming language, which can range from simple commands to complex algorithms. Programmers use their logical thinking and problem-solving skills to create efficient and functional software applications.',\n",
       " 'Machine learning is a subset of artificial intelligence that uses algorithms to analyze and learn from data. It allows computers to identify patterns and make predictions without being explicitly programmed. Machine learning is used in a variety of applications, such as image recognition, natural language processing, and recommendation systems.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch(\n",
    "    [\n",
    "        {\"topic\": \"ChatGPT\"},\n",
    "        {\"topic\": \"Instagram\"},\n",
    "        {\"topic\": \"multimodal\"},\n",
    "        {\"topic\": \"programming\"},\n",
    "        {\"topic\": \"machineLearning\"},\n",
    "    ],\n",
    "    config={\"max_concurrency\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## async stream\n",
    "\n",
    "The function ```chain.astream``` creates an asynchronous stream and processes messages for a given topic asynchronously.\n",
    "\n",
    "It uses an asynchronous for loop (```async for```) to sequentially receive messages from the stream, and the print function to immediately print the contents of the messages (```s.content```). ```end=\"\"``` disables line wrapping after printing, and ```flush=True``` forces the output buffer to be emptied to ensure immediate printing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube is a popular video-sharing platform where users can upload, watch, and interact with videos on a wide variety of topics. It has become a global phenomenon, with billions of users accessing content from around the world every day. The platform also offers opportunities for content creators to monetize their videos through advertising and sponsorships."
     ]
    }
   ],
   "source": [
    "# Use an asynchronous stream to process messages in the 'YouTube' topic.\n",
    "async for token in chain.astream({\"topic\": \"YouTube\"}):\n",
    "    # Print the message content. Outputs directly without newlines and empties the buffer.\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## async invoke\n",
    "\n",
    "The ```ainvoke``` method of a ```chain``` object performs an operation asynchronously with the given arguments. Here, we are passing a dictionary with a key named ```topic``` and a value named ```NVDA``` (NVIDIA's ticker) as arguments. This method can be used to asynchronously request processing for a specific topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the 'NVDA' topic by calling the 'ainvoke' method of the asynchronous chain object.\n",
    "my_process = chain.ainvoke({\"topic\": \"NVDA(ticker)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NVDA is the ticker symbol for Nvidia Corporation, a leading technology company known for its graphics processing units (GPUs) and semiconductor products. The company's products are widely used in gaming, artificial intelligence, and data centers. NVDA has seen significant growth in recent years due to the increasing demand for its products in various industries.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for the asynchronous process to complete.\n",
    "await my_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## async batch\n",
    "\n",
    "The function ```abatch``` batches a series of actions asynchronously.\n",
    "\n",
    "In this example, we are using the ```abatch``` method of the ```chain``` object to asynchronously process actions on ```topic``` .\n",
    "\n",
    "The ```await``` keyword is used to wait for those asynchronous tasks to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs asynchronous batch processing on a given topic.\n",
    "my_abatch_process = chain.abatch(\n",
    "    [{\"topic\": \"YouTube\"}, {\"topic\": \"Instagram\"}, {\"topic\": \"Facebook\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YouTube is a popular video-sharing platform where users can watch, upload, and share videos on a wide range of topics. It offers a diverse range of content, from music videos and tutorials to vlogs and educational videos. Users can also subscribe to channels, like videos, and leave comments to engage with the YouTube community.',\n",
       " 'Instagram is a social media platform where users can share photos and videos with their followers. The app allows users to apply filters and edit their photos before posting. Users can also interact with each other by liking, commenting, and messaging.',\n",
       " 'Facebook is a social media platform that allows users to connect with friends and family, share photos and updates, and follow news and events. Users can create personal profiles, join groups, and engage in conversations through comments and likes. The platform also offers advertising opportunities for businesses to reach a larger audience.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for the asynchronous batch process to complete.\n",
    "await my_abatch_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel\n",
    "\n",
    "Let's take a look at how the LangChain Expression Language supports parallel requests. For example, when you use ```RunnableParallel``` (often written in dictionary form), you execute each element in parallel.\n",
    "\n",
    "Here's an example of running two tasks in parallel using the ```RunnableParallel``` class in the ```langchain_core.runnables``` module.\n",
    "\n",
    "Create two chains (```chain1```, ```chain2```) that use the ```ChatPromptTemplate.from_template``` method to get the capital and area for a given ```country```.\n",
    "\n",
    "These chains are connected via the ```model``` and pipe (```|```) operators, respectively. Finally, we use the ```RunnableParallel``` class to combine these two chains with the keys ```capital``` and ```area``` to create a ```combined``` object that can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Create a chain that asks for the capital of {country}.\n",
    "chain1 = (\n",
    "    PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a chain that asks for the area of {country}.\n",
    "chain2 = (\n",
    "    PromptTemplate.from_template(\"What is the area of {country}?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Create a parallel execution chain that generates the above two chains in parallel.\n",
    "combined = RunnableParallel(capital=chain1, area=chain2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```chain1.invoke()``` function calls the ```invoke``` method of the ```chain1``` object.\n",
    "\n",
    "As an argument, it passes a dictionary with the value ```Canada``` in the key named ```country```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Canada is Ottawa.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run chain1 .\n",
    "chain1.invoke({\"country\": \"Canada\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call ```chain2.invoke()```, this time passing a different country, the ```United States```, for the country key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The total area of the United States is approximately 9.8 million square kilometers (3.8 million square miles).'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run chain2 .\n",
    "chain2.invoke({\"country\": \"USA\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```invoke``` method of the ```combined``` object performs the processing for the given ```country```.\n",
    "\n",
    "In this example, the topic ```USA``` is passed to the ```invoke``` method to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': 'The capital of the United States of America is Washington, D.C.',\n",
       " 'area': 'The total land area of the United States is approximately 3.8 million square miles.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a parallel execution chain.\n",
    "combined.invoke({\"country\": \"USA\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelism in batches\n",
    "\n",
    "Parallelism can be combined with other executable code. Let's try using parallelism with batch.\n",
    "\n",
    "The ```chain1.batch``` function takes a list containing multiple dictionaries as an argument, and processes the values corresponding to the \"topic\" key in each dictionary. In this example, we're batch processing two topics, \"Canada\" and \"United States\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The capital of Canada is Ottawa.',\n",
       " 'The capital of the United States is Washington, D.C.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform batch processing.\n",
    "chain1.batch([{\"country\": \"Canada\"}, {\"country\": \"USA\"}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```chain2.batch``` function takes in multiple dictionaries as a list and performs batch processing.\n",
    "\n",
    "In this example, we request processing for two countries, ```Canada``` and the ```United States```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The total land area of Canada is approximately 9.98 million square kilometers.',\n",
       " 'The total land area of the United States is approximately 3.8 million square miles.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform batch processing.\n",
    "chain2.batch([{\"country\": \"Canada\"}, {\"country\": \"USA\"}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined.batch function is used to process the given data in batches. \n",
    "\n",
    "In this example, it takes a list containing two dictionary objects as arguments and batches data for two countries, Canada and the United States, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'capital': 'The capital of Canada is Ottawa.',\n",
       "  'area': 'The total land area of Canada is approximately 9.98 million square kilometers.'},\n",
       " {'capital': 'The capital of the United States of America is Washington, D.C.',\n",
       "  'area': 'The total land area of the United States is approximately 3.8 million square miles (9.8 million square kilometers).'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processes the given data in batches.\n",
    "combined.batch([{\"country\": \"Canada\"}, {\"country\": \"USA\"}])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-xFIp9mn7-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
